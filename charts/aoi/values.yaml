# Global definitions used for components directly managed by this chart
global:
  clusterDomain: "cluster.local"
  tsdb:
    high_availability:
      # -- Enable high-availability for tsdb (Victoria-metrics-single)
      enabled: false
  # -- Default priorityClassName to use
  priorityClassName: null
  serviceLabels: {}
  serviceAnnotations: {}
  image:
    pullPolicy: Always
  revisionHistoryLimit: 5
  annotations: {}
  podAnnotations: {}
  podLabels: {}
  imagePullSecrets: []
  containerSecurityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - all

# Namespace for tenant dashboards
dashboards:
  # -- Options to configure the bootstrapConfig used for tenant dashboard namespace
  bootstrapConfig:
    git:
      github:
        secretRef: tcs-github-auth
        template:
          adminTeam: oaas-team
          owner: neticdk-k8s
          repo: tenant-grafana-template
    vault: {}
    externalSecretsStore: {}
  # -- Options to configure the projectBootstrap used for tenant dashboard namespace
  projectBootstrap:
    git: {}

# Prometheus Queries are filtered using client JWT token (Grafana)
authProxy:
  # -- Enable filtering of Prometheus Queries based on client JWT token (Grafana)
  enabled: true
  replicas: 1
  image:
    registry: registry.netic.dk
    repository: netic-oaas/cortex-proxy
    tag: v1.0.4
  priorityClassName: null
  annotations: {}
  podAnnotations: {}
  podLabels: {}
  selectorLabels: {}
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
    runAsNonRoot: true
  service:
    annotations: {}
    labels: {}
  extraArgs: []
  extraEnv: []
  extraEnvFrom: []
  resources:
    limits:
      memory: 64Mi
    requests:
      cpu: 100m
      memory: 64Mi
  terminationGracePeriodSeconds: 30
  affinity: []
  nodeSelector: {}
  topologySpauthProxyConstraints: []
  tolerations: []
  podManagementPolicy: "Parallel"


prometheus:
  image:
    registry: docker.io
    repository: victoriametrics/vmagent
    tag: v1.91.2
  resources:
    limits:
      memory: 768Mi
    requests:
      cpu: 100m
      memory: 256Mi
  priorityClassName: null
  podAnnotations: {}
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
    runAsNonRoot: true
  # -- relabel configs to apply to samples before ingestion.
  relabelConfig: |
    - source_labels: [cluster_id, namespace]
      separator: _
      regex: (.*)
      target_label: namespace_id
      replacement: $1
      action: replace
  persistence:
    size: 60Gi
  configReloader:
    resources:
      limits:
        memory: 25Mi
      requests:
        cpu: 10m
        memory: 25Mi
  extraVolumes: []
  extraVolumeMounts: []

# Used in HA mode to route request to one or the other tsdb - https://github.com/jacksontj/promxy?tab=readme-ov-file#why-promxy
promxy:
  replicas: 1
  image:
    registry: quay.io
    repository: jacksontj/promxy
    tag: v0.0.84
  priorityClassName: null
  annotations: {}
  podAnnotations: {}
  podLabels: {}
  selectorLabels: {}
  podSecurityContext:
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 2000
    runAsNonRoot: true
  service:
    annotations: {}
    labels: {}
  extraArgs: []
  extraEnv: []
  extraEnvFrom: []
  resources:
    limits:
      memory: 64Mi
    requests:
      cpu: 50m
      memory: 64Mi
  terminationGracePeriodSeconds: 30
  affinity: []
  nodeSelector: {}
  topologySpauthProxyConstraints: []
  tolerations: []
  podManagementPolicy: "Parallel"
  config: |
    ##
    ### Promxy configuration
    ##
    promxy:
      server_groups:
        - static_configs:
            - targets:
              - victoria-metrics-single-1-server.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:8428
          labels:
            replica: 1
          http_client:
            dial_timeout: 1s
          ignore_error: true
          remote_read: true
          remote_read_path: /api/v1
        - static_configs:
            - targets:
              - victoria-metrics-single-2-server.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:8428
          labels:
            replica: 2
          http_client:
            dial_timeout: 1s
          ignore_error: true
          remote_read: true
          remote_read_path: /api/v1


externalSecret:
  vaultServer: null
  vaultPath: null
  vaultMountPath: null
  vaultDataFromKey: null


grafana:
  # -- If true deploy Grafana for tenant dashboards
  enabled: true
  image:
    pullPolicy: Always
  testFramework:
    enabled: false
  priorityClassName: "secure-cloud-stack-tenant-namespace-application-critical"
  resources:
    limits:
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 256Mi
  podPortName: http
  # sidecar used to provision tenant dashboards found as configmaps in the namespace application-operations-dashboards with label aoi_dashboard
  sidecar:
    image:
      pullPolicy: Always
    dashboards:
      enabled: true
      # -- Load configmaps with label key
      label: aoi_dashboard
      # -- Watch for configmaps in namespaces
      searchNamespace:
        - application-operations-dashboards
      # -- override grafana folder using annotation
      folderAnnotation: grafana_dashboard_folder
      provider:
        disableDelete: true
        foldersFromFilesStructure: true
    datasources:
      enabled: true
      label: aoi_grafana_datasource
  ingress:
    enabled: false
    fqdn: null


victoria-metrics-single-1:
  rbac:
    create: false
  server:
    # -- Data retention period
    retentionPeriod: 90d
    image:
      pullPolicy: Always
    # Try to not schedule victoria-metrics-single-1 in same zone as victoria-metrics-single-2
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                    - victoria-metrics-single-2
            topologyKey: topology.kubernetes.io/zone
    persistentVolume:
      # -- Size of the volume. Should be calculated based on the metrics you send and retention policy you set.
      size: 100Gi
    resources:
      limits:
        memory: 1024Mi
      requests:
        cpu: 200m
        memory: 1024Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
          - all
    podSecurityContext:
      runAsUser: 1000
      runAsGroup: 3000
      fsGroup: 2000
    serviceMonitor:
      enabled: true
      extraLabels:
        netic.dk/monitoring: "true"

victoria-metrics-single-2:
  rbac:
    create: false
  server:
    # -- Data retention period
    retentionPeriod: 90d
    image:
      pullPolicy: Always
    # Try to not schedule victoria-metrics-single-2 in same zone as victoria-metrics-single-1      
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                    - victoria-metrics-single-1
            topologyKey: topology.kubernetes.io/zone
    persistentVolume:
      # -- Size of the volume. Should be calculated based on the metrics you send and retention policy you set.
      size: 100Gi
    resources:
      limits:
        memory: 1024Mi
      requests:
        cpu: 200m
        memory: 1024Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
          - all
    podSecurityContext:
      runAsUser: 1000
      runAsGroup: 3000
      fsGroup: 2000
    serviceMonitor:
      enabled: true
      extraLabels:
        netic.dk/monitoring: "true"

alerting:
  # -- Enable deploying alerting components
  enabled: false
  # -- Value of the label (cluster_id)
  clusterId: ""
  clusterWideNamespace:
    # -- Create alerting namespace for cluster-wide alert definitions
    enabled: false
    name: application-operations-alerting
    # -- Options to configure the bootstrapConfig used for cluster-wide alert namespace.
    bootstrapConfig:
      git:
        github:
          secretRef: tcs-github-auth
          template:
            adminTeam: oaas-team
            owner: neticdk-k8s
            repo: tenant-alerting-template
      vault: {}
      externalSecretsStore: {}
    # -- Options to configure the projectBootstrap used for cluster-wide alert namespace.
    projectBootstrap:
      git: {}
  # -- List of namespaces which should have alerting components deployed
  namespaces: []
  # -- Override the default helmRepository used to deploy alerting components
  helmRepository: null
  # -- Values to configure for the victoria-metrics-alert helm chart. https://github.com/VictoriaMetrics/helm-charts/blob/master/charts/victoria-metrics-alert/values.yaml
  helmRelease:
    values:
      server:
        priorityClassName: "secure-cloud-stack-tenant-namespace-application-critical"
        image:
          registry: docker.io
          repository: victoriametrics/vmalert
          pullPolicy: Always
        resources:
          limits:
            memory: 64Mi
          requests:
            cpu: 10m
            memory: 64Mi
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
              - all
        configReloader:
          image:
            registry: docker.io
            repository: kiwigrid/k8s-sidecar
            tag: "1.25.2"
            pullPolicy: Always
          resources:
            limits:
              memory: 96Mi
            requests:
              cpu: 10m
              memory: 96Mi
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            capabilities:
              drop:
                - ALL
        podSecurityContext:
          runAsUser: 1000
          runAsGroup: 3000
          fsGroup: 2000
      alertmanager:
        priorityClassName: "secure-cloud-stack-tenant-namespace-application-critical"
        image:
          registry: docker.io
          repository: prom/alertmanager
          pullPolicy: Always
        resources:
          limits:
            memory: 64Mi
          requests:
            cpu: 10m
            memory: 64Mi
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
              - ALL
        podSecurityContext:
          runAsUser: 1000
          runAsGroup: 3000
          fsGroup: 2000

# This is here to disable everything from the victoria-metrics-alert helm chart in chart.yaml since it is only used for versioning 
# Do not change!
victoria-metrics-alert:
  serviceAccount:
    create: false
  rbac:
    create: false
  server:
    enabled: false
    configMap: "null"
